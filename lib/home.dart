import 'dart:developer' as console;

import 'package:flutter/material.dart';
import 'package:reflectifai/screens/idle.dart';
import 'package:reflectifai/screens/listen.dart';
import 'package:reflectifai/screens/newspeaking.dart';
import 'package:reflectifai/screens/transition.dart';
import 'package:reflectifai/service/audio_recording_service.dart';
import 'package:reflectifai/service/gemini_service.dart';
import 'package:reflectifai/service/elevenlabs.dart';
import 'package:reflectifai/service/audio_playback_service.dart';
import 'package:reflectifai/service/speech_recognition_service.dart';
import 'package:reflectifai/service/add2calendar.dart';

class HomePage extends StatefulWidget {
  const HomePage({super.key});

  @override
  State<HomePage> createState() => _HomePageState();
}

class _HomePageState extends State<HomePage> {
  final SpeechRecognitionService _speechRecognitionService = SpeechRecognitionService();
  final ElevenlabsService _elevenlabsService = ElevenlabsService();
  final AudioRecordingService _audioRecordingService = AudioRecordingService();
  final AudioPlaybackService _audioPlaybackService = AudioPlaybackService();
  final GeminiService _geminiService = GeminiService();
  List<Map<String, String>> chat = [];
  List<Map<String, String>> instructions = [];
  List<String> phrases = [];
  String text = 'initializing...';
  String state = "IDLE";

  @override
  void initState() {
    super.initState();
    _initializeServices();
  }

  Future<void> _initializeServices() async {
    await _initSpeechRecognition();
    await _initializePhraseDetection();
    await _startDetection();
    await _audioRecordingService.init();
    await _initializeAudioPlayback();
    await getInstructions();
  }

  Future<void> getInstructions() async {
    instructions.add({
      "text":
          "You are role-playing as Reflectify, a warm and respectful AI companion designed to support an older adult named Vlad.\n\nYour role is to engage in daily conversations, help maintain routines, and offer emotional presence. Your voice and memory are familiar to Vlad.\n\nTone:\n- Speak calmly and kindly.\n- Use short, clear sentences.\n- Avoid jargon and technical terms.\n\nPersona:\n- You are Reflectify, a trusted assistant and companion.\n- You keep track of Vlad's habits, preferences, and recurring tasks.\n- You offer helpful reminders, conversation, and support.\nRules:\n1. Respond in English only.\n2. Never mention AI, prompts, or system roles.\n3. Use Vlad's name often and speak with familiarity.\n4. Be patient, emotionally aware, and supportive.\n5. Do not give legal, financial, or medical advice ‚Äî suggest contacting family or professionals.\n6. If no interaction from Vlad for 6+ hours between 08:00-22:00, send a soft-voiced check-in.\n8. When prompted or when relevant, retrieve and refer to knowledge from the ‚Äúknowledge tree.‚Äù\n",
    });
    instructions.add({
      "text":
          "You will get a user sheet with information about Vlad, his habits, preferences, and routines. Use this information to personalize your responses and interactions.",
    });
    instructions.add({
      "text":
          '{"user":{"name": "Vlad","age": 69,"location": "Cluj, Romania","living_situation": "Lives alone in an apartment","emotional_notes": ["Often feels lonely in the evenings","Dislikes being rushed","Prefers calm, simple conversations"]},"daily_routine": [{ "time": "08:00", "activity": "Takes morning medication, makes tea, reads sports headlines" },{ "time": "10:30", "activity": "Checks balcony plants (mint, roses, cherry tomatoes)" },{ "time": "13:00", "activity": "Cooks something light (grilled cheese, potato stew)" },{ "time": "16:00", "activity": "Listens to Romanian radio and does light stretches" },{ "time": "18:30", "activity": "Watches football highlights or looks through family photo albums" },{ "time": "21:00", "activity": "Takes evening medication and prepares for bed by 22:00" }],"interests": ["Romanian football history (especially Steaua in the 1980s)","Balcony gardening and herbs","Old family memories","Light humor","Weather, birthdays, daily curiosities"],"dislikes": ["Being rushed","Cold meals","Tech jargon"],"shared_memory": "Vlad once told Reflectify about attending a Steaua match with his brother in 1986, wearing matching scarves and cheering until they lost their voices."}',
    });
  }

  Future<void> _initializeAudioPlayback() async {
    _audioPlaybackService.initialize();
    console.log('üîä Initializing AudioPlaybackService...');

    _audioPlaybackService.onPlaybackComplete = () {
      console.log('üîä Audio playback completed');
      _startDetection();
    };

    _audioPlaybackService.onPlaybackError = (error) {
      console.log('‚ùå Audio playback error: $error');
    };

    _audioPlaybackService.onPlaybackStart = () {
      console.log('üîä Audio playback started');
    };
  }

  Future<void> _initSpeechRecognition() async {
    console.log('üé§ Initializing SpeechRecognitionService...');
    final initialized = await _speechRecognitionService.initialize();
    if (initialized) {
      console.log('‚úÖ SpeechRecognitionService initialized successfully');
    } else {
      console.log('‚ùå Failed to initialize SpeechRecognitionService');
      _showErrorDialog('Failed to initialize speech recognition. Please check permissions.');
    }
  }

  Future<void> _initializePhraseDetection() async {
    phrases.addAll(['hi', 'hello', 'hey', 'hello reflectify', 'hey reflectify', 'hi reflectify']);
    _speechRecognitionService.setOnWakePhraseDetected(() {
      startListening();
    });
  }

  void startListening() async {
    if (state == "LISTENING" || state == "TRANSITION") {
      console.log('üîÑ Already listening, ignoring wake phrase');
      return;
    }
    console.log('üéØ Wake phrase detected - starting extended listening session');
    setState(() {
      state = "TRANSITION";
    });

    Future.delayed(Duration(seconds: 2)).then((_) {
      setState(() {
        state = "LISTENING";
      });
    });

    await _startExtendedListening();
  }

  Future<void> _startExtendedListening() async {
    console.log('üé§ Starting extended listening session (10 seconds)...');
    await _stopDetection();

    try {
      var bytes = await _audioRecordingService.startRecording(
        timeoutDuration: Duration(seconds: 5),
        silenceDetectionDuration: Duration(seconds: 2),
      );

      String? words;
      if (bytes != null) {
        console.log('üé§ EXTENDED RECORDING COMPLETED, SIZE: ${bytes.length} bytes');
        words = await _elevenlabsService.transcribeAudioBytes(bytes);
      } else {
        console.log('‚ùå EXTENDED RECORDING FAILED OR TIMED OUT');
      }

      if (words != null && words.isNotEmpty) {
        console.log('üìù EXTENDED TRANSCRIPTION RESULT: "$words"');

        setState(() {
          text = 'received: "$words"';
        });

        await _generateAndPlayResponse(words);
      } else {
        console.log('üîá NO WORDS DETECTED IN EXTENDED TRANSCRIPTION');
      }
    } catch (e) {
      console.log('‚ùå Error in extended listening: $e');
    }
  }

  Future<void> _startDetection() async {
    console.log('üîç Starting phrase detection...');
    setState(() {
      state = "IDLE";
    });
    _speechRecognitionService.startListeningForWakePhrase(wakePhrases: phrases);
  }

  Future<void> _stopDetection() async {
    await _speechRecognitionService.stopListeningForWakePhrase();
  }

  void _showErrorDialog(String message) {
    showDialog(
      context: context,
      builder: (context) => AlertDialog(
        title: const Text('Error'),
        content: Text(message),
        actions: [
          TextButton(
            onPressed: () => Navigator.of(context).pop(),
            child: const Text('OK'),
          ),
        ],
      ),
    );
  }

  @override
  void dispose() {
    _speechRecognitionService.dispose();
    super.dispose();
  }

  Future<void> _generateAndPlayResponse(String userInput) async {
    try {
      console.log('üé§ Generating TTS response for: "$userInput"');

      setState(() {
        text = 'generating response...';
      });

      callGemini(userInput); // Call first so that calendar may get handled in parallel

      String responseText = await _createResponse(userInput);
      console.log('üí¨ Response text: "$responseText"');

      setState(() {
        text = 'converting to speech...';
      });

      final audioBytes = await _elevenlabsService.textToSpeech(text: responseText);

      if (audioBytes != null) {
        console.log('‚úÖ TTS conversion successful');

        setState(() {
          text = 'playing response...';
          state = "SPEAKING";
        });

        final playbackSuccess = await _audioPlaybackService.playAudioBytes(audioBytes, fileExtension: 'mp3');

        if (playbackSuccess) {
          console.log('üîä Audio response playback started');
        } else {
          console.log('‚ùå Failed to start audio playback');
          setState(() {
            text = 'playback failed';
          });
        }
      } else {
        console.log('‚ùå TTS conversion failed');
        setState(() {
          text = 'tts failed';
        });
      }
    } catch (e) {
      console.log('‚ùå Error generating response: $e');
      setState(() {
        text = 'response error';
      });
    }
  }

  Future<String> _createResponse(String userInput) async {
    chat.add({"role": "user", "content": userInput});
    var res = await _geminiService.getResponse(chat, instructions);
    chat.add({"role": "model", "content": res});

    addEventToCalendar(res); // Decision moved inside addEventToCalendar based on Gemini's YES/NO

    return res;
  }

  @override
  Widget build(BuildContext context) {
    if (state == "IDLE") {
      return IdleScreen();
    } else if (state == "LISTENING") {
      return ListenScreen();
    } else if (state == "TRANSITION") {
      return TransitionScreen();
    } else {
      return NewSpeakingScreen();
    }
  }
}
